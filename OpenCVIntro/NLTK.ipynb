{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a string with punctuation\n"
     ]
    }
   ],
   "source": [
    "str = \"This is[] an exam*ple of a# str&ing $with pun@ctuatio*n.\"\n",
    "result = str.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "inp = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "tok = word_tokenize(inp)\n",
    "result = [i for i in tok if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "input_str = \"There are several types of stemming algorithms.\"\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))\n",
    "    \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "input_str = \"been had done languages cities mice\"\n",
    "input_str = word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Parts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('examples', 'NNS'),\n",
       " (':', ':'),\n",
       " ('an', 'DT'),\n",
       " ('article', 'NN'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('write', 'VB'),\n",
       " (',', ','),\n",
       " ('interesting', 'VBG'),\n",
       " (',', ','),\n",
       " ('easily', 'RB'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " (',', ','),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "text = word_tokenize(input_str)\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = open('Data/big.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = str.translate(str.maketrans(\"\",\"\",string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'adventure', 'sherlock', 'holmes', 'sir', 'arthur', 'conan', 'doyle', '15', 'series', 'sir', 'arthur', 'conan', 'doyle', 'copyright', 'law', 'changing', 'world', 'sure', 'check', 'copyright', 'law', 'country', 'downloading', 'redistributing', 'project', 'gutenberg', 'ebook', 'header', 'first', 'thing', 'seen', 'viewing', 'project', 'gutenberg', 'file', 'please', 'remove', 'change', 'edit', 'header', 'without', 'written', 'permission', 'please', 'read', 'legal', 'small', 'print', 'information', 'ebook', 'project', 'gutenberg', 'bottom', 'file', 'included', 'important', 'information', 'specific', 'right', 'restriction', 'file', 'may', 'used', 'also', 'find', 'make', 'donation', 'project', 'gutenberg', 'get', 'involved', 'welcome', 'world', 'free', 'plain', 'vanilla', 'electronic', 'text', 'ebooks', 'readable', 'human', 'computer', 'since', '1971', 'ebooks', 'prepared', 'thousand', 'volunteer', 'title', 'adventure', 'sherlock', 'holmes', 'author', 'sir', 'arthur', 'conan', 'doyle', 'release', 'date', 'march', '1999', 'ebook', '1661', 'recently', 'updated', 'november', '29', '2002', 'edition', '12', 'language', 'english', 'character', 'set', 'encoding', 'ascii', 'start', 'project', 'gutenberg', 'ebook', 'adventure', 'sherlock', 'holmes', 'additional', 'editing', 'jose', 'menendez', 'adventure', 'sherlock', 'holmes', 'sir', 'arthur', 'conan', 'doyle', 'content', 'scandal', 'bohemia', 'ii', 'redheaded', 'league', 'iii', 'case', 'identity', 'iv', 'boscombe', 'valley', 'mystery', 'v', 'five', 'orange', 'pip', 'vi', 'man', 'twisted', 'lip', 'vii', 'adventure', 'blue', 'carbuncle', 'viii', 'adventure', 'speckled', 'band', 'ix', 'adventure', 'engineer', 'thumb', 'x', 'adventure', 'noble', 'bachelor', 'xi', 'adventure', 'beryl', 'coronet', 'xii', 'adventure', 'copper', 'beech', 'adventure', 'scandal', 'bohemia', 'sherlock', 'holmes', 'always', 'woman', 'seldom', 'heard', 'mention', 'name', 'eye', 'eclipse', 'predominates', 'whole', 'sex', 'felt', 'emotion']\n"
     ]
    }
   ],
   "source": [
    "tok = word_tokenize(str)\n",
    "result = [lemmatizer.lemmatize(i) for i in tok if not i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "c = Counter(result)\n",
    "c = sorted(c.items(), key=lambda c: -c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "z = 1\n",
    "for x in c:\n",
    "    dict[x[0]] = z\n",
    "    z+=1\n",
    "# dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "for sentence in sent_text:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokenized_text)\n",
    "    print(tagged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
